# -*- coding: utf-8 -*-
"""wod1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1odEyDQQK3t7usTTRSEzy_d2qVhmUMklO

# Math Question Answer Verification Competition

## Starter Code

Borrowed from [official Unsloth implementation](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing#scrollTo=MKX_XKs_BNZR)
"""

# %%capture
# This cell will take time
!pip install unsloth

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

"""## Load model and wrap with LoRA adapters"""

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

"""## Competition dataset"""

# download and load competition dataset

from datasets import load_dataset
dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp")
# print and see dataset
dataset

prompt = """You are a skilled mathematician responsible for determining the correctness of answers to math questions. Carefully analyze the question, answer, and explanation provided. Respond with 'True' if the answer is correct, otherwise 'False'.

### Question:
{}

### Answer:
{}

### Explanation:

### Output:
{}"""


EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    question = examples["question"]
    ans       = examples["answer"]
    output      = examples["is_correct"]
    texts = []
    for instruction, input, output in zip(question, ans, output):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

# Process the training dataset and generate prompt for each datapoint

train_dataset = dataset['train'].map(formatting_prompts_func, batched = True,)

#print a smaple training example
train_dataset['text'][0]

"""## SFT"""

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported


training_args = TrainingArguments(
    per_device_train_batch_size=4,  # 尝试增大批次大小
    gradient_accumulation_steps=8,  # 增加累计步数
    warmup_steps=100,               # 提高热身步数以获得更稳定的学习过程
    num_train_epochs = 3,
    max_steps=1000,                 # 增加训练步数
    learning_rate=1e-4,             # 降低学习率
    fp16=not is_bfloat16_supported(),
    bf16=is_bfloat16_supported(),
    logging_steps=10,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir="outputs",
    report_to="none",
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 4,
    packing = False, # Can make training 5x faster for short sequences.
    args = training_args
)

trainer_stats = trainer.train()

from sklearn.metrics import accuracy_score
import pandas as pd
import random

# 划分一个小的 `pre-test` 集合
sample_size = 100
pre_test_dataset = train_dataset.shuffle(seed=42).select(range(sample_size))

# 开启推理模式
FastLanguageModel.for_inference(model)

# 存储 `pre-test` 集合上的预测结果
pre_test_results = []

# 遍历 `pre-test` 集合中的每个问题和答案
for idx, (question, answer, true_label) in enumerate(zip(pre_test_dataset['question'], pre_test_dataset['answer'], pre_test_dataset['is_correct'])):
    # 构建输入提示
    input_prompt = prompt.format(question, answer, "")

    # 编码输入
    inputs = tokenizer([input_prompt], return_tensors="pt").to("cuda")

    # 生成预测
    input_token_len = inputs['input_ids'].shape[1]
    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)

    # 解码生成的文本
    response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)[0].strip()

    # 解析模型输出为 True 或 False
    predicted_label = response.lower() == 'true'

    # 将结果添加到列表中
    pre_test_results.append({
        "ID": idx,
        "true_label": true_label,
        "predicted_label": predicted_label
    })

# 转换结果为 DataFrame
pre_test_results_df = pd.DataFrame(pre_test_results)

# 计算并输出 `pre-test` 集合上的准确率
accuracy = accuracy_score(pre_test_results_df["true_label"], pre_test_results_df["predicted_label"])
print(f"Pre-test accuracy: {accuracy * 100:.2f}%")

"""## inference"""

test_dataset = dataset['test']

import pandas as pd
import os
from tqdm import tqdm  # 导入 tqdm 库

# Enable inference mode
FastLanguageModel.for_inference(model)

# Create an empty list to store all prediction results
results = []

# 使用 tqdm 包装测试集迭代器以显示进度条
for idx, (question, answer) in tqdm(enumerate(zip(test_dataset['question'], test_dataset['answer'])), total=len(test_dataset['question']), desc="Processing Inference"):
    # Create the input prompt
    input_prompt = prompt.format(question, answer, "")

    # Encode the input
    inputs = tokenizer([input_prompt], return_tensors="pt").to("cuda")

    # Generate predictions
    input_token_len = inputs['input_ids'].shape[1]
    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)

    # Decode the generated text
    response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)[0].strip()

    # Interpret the model output as True or False
    is_correct = response.lower() == 'true'

    # Append the result to the list
    results.append({"ID": idx, "is_correct": is_correct})

# Convert all results to a DataFrame and save as a CSV file
results_df = pd.DataFrame(results)
file_path = "submission.csv"
results_df.to_csv(file_path, index=False)

# Print the file path
absolute_path = os.path.abspath(file_path)
print(f"Results saved to {absolute_path}")

"""## saving model"""

model.save_pretrained("lora_model") # Local saving
tokenizer.save_pretrained("lora_model")

if True:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference